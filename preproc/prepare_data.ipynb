{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a214425f-fdfa-4902-8a90-140a8ea2619b",
   "metadata": {},
   "source": [
    "# This is the Preprocessing codes to analyze the running invervention dataset\n",
    "\n",
    "## Gitrepo\n",
    "    https://github.com/Vincent-wq/interventionMDD\n",
    "    \n",
    "## Dataset:\n",
    "    0. Data source: [https://openneuro.org/datasets/ds003799/versions/1.0.0]( Openneuro)\n",
    "    1. Imaging data MRI *3 sessions;\n",
    "    2. Depression measures: Description: German version of the Center for Epidemiological Studies Depression Scale(CES-D). Hautzinger, M., Bailer, M., Hofmeister, D., & Keller, F. (2012). Allgemeine Depressionsskala (ADS). Manual (2. Auflage). Göttingen: Hogrefe Verlag GmbH. This self-assessing scale comprises 20 items covering depressive symptoms in the emotional, motivational, cognitive, somatic, and motoric domain. Participants had to indicate the extent to which the given conditions apply to them in the last week on a four-point scale ranging from “infrequent” to “the most time”.\n",
    "\n",
    "## Preprocessing steps:\n",
    "    1. Pre-registration: https://osf.io/yg5bu/\n",
    "    2. Preprocessing steps: \n",
    "        2.1 fMRIPrep 20.2.7\n",
    "        2.2 QA\n",
    "        2.3 TBD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4f0d0f-c916-47b9-8b36-b2df839db790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs and envs\n",
    "import sys\n",
    "proj_path_str='/scratch/interventionMDD'\n",
    "sys.path.append(proj_path_str)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# main PATH\n",
    "main_dir = Path(proj_path_str)\n",
    "data_dir = main_dir / 'data' \n",
    "fig_dir =  main_dir / 'figs' \n",
    "\n",
    "# read tabular files \n",
    "participant_file = data_dir / 'participants.tsv'  # Inormation from download database.\n",
    "mdd_score_file   = data_dir / 'phenotype_CES-D.tsv'  # Inormation from download database.\n",
    "\n",
    "# read and select UPDRS-3data\n",
    "sub_data = pd.read_csv(participant_file, sep='\\t')\n",
    "sub_data.index=sub_data['participant_id']\n",
    "sub_data=sub_data.drop(columns=['participant_id']).copy()\n",
    "sub_col_list = ['participant_id', 'group', 'age', 'sex', 'size', 'weight'];\n",
    "\n",
    "# read and select MOCA data\n",
    "mdd_data  = pd.read_csv(mdd_score_file, sep='\\t')\n",
    "mdd_data.index = mdd_data['participant_id']\n",
    "mdd_data=mdd_data.drop(columns=['participant_id']).copy()\n",
    "moca_list = ['participant_id', 'CES-D_1', 'CES-D_2', 'CES-D_3'];\n",
    "\n",
    "##join data\n",
    "data_df = sub_data.join(mdd_data, on = ['participant_id'], how='left').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d73523-be13-422a-90c4-ed9d936ee3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freesurfer brain measures\n",
    "fs_ses1_dir  =  main_dir / \"scripts\" / \"collect_fs_measures\" / \"ses-1\" \n",
    "fs_ses2_dir  =  main_dir / \"scripts\" / \"collect_fs_measures\" / \"ses-2\" \n",
    "fs_ses3_dir  =  main_dir / \"scripts\" / \"collect_fs_measures\" / \"ses-3\" \n",
    "\n",
    "files_2_read={'seg'      : ['aseg_stats.txt', 'wmparc_stats.txt'],\n",
    "              'Destrieux': {'ct': '.a2009s.thickness.txt',  'area':'.a2009s.area.txt',   'volume':'.a2009s.volume.txt'},\n",
    "              'DKT'      : {'ct': '.DKTatlas.thickness.txt','area':'.DKTatlas.area.txt', 'volume':'.DKTatlas.volume.txt'}}\n",
    "\n",
    "for i_group in range(n_groups):\n",
    "    \"\"\"\n",
    "    Loop to gather all the freesurfer outputs for ET/PD/NC MNI group.\n",
    "    Output: all_data (dataframe).\n",
    "    \"\"\"\n",
    "    print('Reading ' , GROUPS[i_group], ' freesurfer stats data...')\n",
    "    raw_data_path = FS_DIR/GROUPS[i_group]\n",
    "    \n",
    "    # preparing files\n",
    "    subcortical_file = raw_data_path / (files_2_read['seg'][0]); wm_file = raw_data_path / (files_2_read['seg'][1]);\n",
    "    # Des parcellation\n",
    "    lh_Des_ct_file = raw_data_path / ('lh'+files_2_read['Destrieux']['ct']);  rh_Des_ct_file = raw_data_path /  ('rh'+files_2_read['Destrieux']['ct']);\n",
    "    lh_Des_vol_file = raw_data_path / ('lh'+files_2_read['Destrieux']['volume']); rh_Des_vol_file = raw_data_path / ('rh'+files_2_read['Destrieux']['volume']);\n",
    "    lh_Des_area_file = raw_data_path / ('lh'+files_2_read['Destrieux']['area']); rh_Des_area_file = raw_data_path / ('rh'+files_2_read['Destrieux']['area']);\n",
    "    lh_Des_meancurv_file = raw_data_path / ('lh'+files_2_read['Destrieux']['meancurv']); rh_Des_meancurv_file = raw_data_path / ('rh'+files_2_read['Destrieux']['meancurv']);\n",
    "    # DKT parcellation\n",
    "    lh_DKT_area_file = raw_data_path / ('lh'+files_2_read['DKT']['area']);       rh_DKT_area_file = raw_data_path / ('rh'+files_2_read['DKT']['area']);\n",
    "    lh_DKT_ct_file = raw_data_path / ('lh'+files_2_read['DKT']['ct']);        rh_DKT_ct_file = raw_data_path /  ('rh'+files_2_read['DKT']['ct']);\n",
    "    lh_DKT_vol_file = raw_data_path / ('lh'+files_2_read['DKT']['volume']);       rh_DKT_vol_file = raw_data_path / ('rh'+files_2_read['DKT']['volume']);\n",
    "    lh_DKT_meancurv_file = raw_data_path / ('lh'+files_2_read['DKT']['meancurv']);       rh_DKT_meancurv_file = raw_data_path / ('rh'+files_2_read['DKT']['meancurv']);\n",
    "    ## drop_list\n",
    "    aseg_drop = [\"EstimatedTotalIntraCranialVol\"]; \n",
    "    wm_drop = [\"MaskVol\", \"EstimatedTotalIntraCranialVol\", \"CerebralWhiteMatterVol\", \"rhCerebralWhiteMatterVol\", \"lhCerebralWhiteMatterVol\"];\n",
    "    parc_drop = [\"BrainSegVolNotVent\", \"eTIV\"]; \n",
    "    sub_list[i_group].loc[:, 'site']='mni_001'\n",
    "    sub_list[i_group].loc[:, 'group'] = sub_list[i_group].loc[:, 'diagnosis'];\n",
    "    # read files\n",
    "    subcortical_tab = pd.read_csv(subcortical_file, sep='\\t', header=0, index_col=0); \n",
    "    subcortical_tab['eTIV']=subcortical_tab['EstimatedTotalIntraCranialVol']\n",
    "    subcortical_tab.drop(aseg_drop, axis=1, inplace=True);\n",
    "    # site\n",
    "    res = sub_list[i_group].join(subcortical_tab, how='left');\n",
    "    wm_tab = pd.read_csv(wm_file, sep='\\t', header=0, index_col=0); wm_tab.drop(wm_drop, axis=1, inplace=True);\n",
    "    res1   = res.join(wm_tab, how='left');\n",
    "    # read Des/DKT parcelation data\n",
    "    lh_Des_ct_tab  = pd.read_csv(lh_Des_ct_file,  sep='\\t', header=0, index_col=0);       lh_Des_ct_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_Des_ct_tab  = pd.read_csv(rh_Des_ct_file,  sep='\\t', header=0, index_col=0);       rh_Des_ct_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_Des_vol_tab = pd.read_csv(lh_Des_vol_file, sep='\\t', header=0, index_col=0);       lh_Des_vol_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_Des_vol_tab = pd.read_csv(rh_Des_vol_file, sep='\\t', header=0, index_col=0);       rh_Des_vol_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_Des_area_tab = pd.read_csv(lh_Des_area_file, sep='\\t', header=0, index_col=0);     lh_Des_area_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_Des_area_tab = pd.read_csv(rh_Des_area_file, sep='\\t', header=0, index_col=0);     rh_Des_area_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_Des_curv_tab = pd.read_csv(lh_Des_meancurv_file, sep='\\t', header=0, index_col=0); lh_Des_curv_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_Des_curv_tab = pd.read_csv(rh_Des_meancurv_file, sep='\\t', header=0, index_col=0); rh_Des_curv_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    # DKT atlas\n",
    "    lh_DKT_ct_tab  = pd.read_csv(lh_DKT_ct_file,  sep='\\t', header=0, index_col=0);       lh_DKT_ct_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_DKT_ct_tab  = pd.read_csv(rh_DKT_ct_file,  sep='\\t', header=0, index_col=0);       rh_DKT_ct_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_DKT_vol_tab = pd.read_csv(lh_DKT_vol_file, sep='\\t', header=0, index_col=0);       lh_DKT_vol_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_DKT_vol_tab = pd.read_csv(rh_DKT_vol_file, sep='\\t', header=0, index_col=0);       rh_DKT_vol_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_DKT_area_tab = pd.read_csv(lh_DKT_area_file, sep='\\t', header=0, index_col=0);     lh_DKT_area_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_DKT_area_tab = pd.read_csv(rh_DKT_area_file, sep='\\t', header=0, index_col=0);     rh_DKT_area_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    lh_DKT_curv_tab = pd.read_csv(lh_DKT_meancurv_file, sep='\\t', header=0, index_col=0); lh_DKT_curv_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    rh_DKT_curv_tab = pd.read_csv(rh_DKT_meancurv_file, sep='\\t', header=0, index_col=0); rh_DKT_curv_tab.drop(parc_drop, axis=1, inplace=True);\n",
    "    # merge Des/DKT parcelation data\n",
    "    seg_Des_tab=       res1.join(lh_Des_ct_tab, how='left');  seg_Des_tab=seg_Des_tab.join(rh_Des_ct_tab, how='left'); \n",
    "    seg_Des_tab=seg_Des_tab.join(lh_Des_vol_tab,how='left');  seg_Des_tab=seg_Des_tab.join(rh_Des_vol_tab,how='left'); \n",
    "    seg_Des_tab=seg_Des_tab.join(lh_Des_area_tab,how='left'); seg_Des_tab=seg_Des_tab.join(rh_Des_area_tab,how='left');\n",
    "    seg_Des_tab=seg_Des_tab.join(lh_Des_curv_tab,how='left'); seg_Des_tab=seg_Des_tab.join(rh_Des_curv_tab,how='left');\n",
    "    \n",
    "    seg_DKT_tab=res1.join(lh_DKT_ct_tab, how='left');         seg_DKT_tab=seg_DKT_tab.join(rh_DKT_ct_tab, how='left'); \n",
    "    seg_DKT_tab=seg_DKT_tab.join(lh_DKT_vol_tab,how='left');  seg_DKT_tab=seg_DKT_tab.join(rh_DKT_vol_tab,how='left'); \n",
    "    seg_DKT_tab=seg_DKT_tab.join(lh_DKT_area_tab,how='left'); seg_DKT_tab=seg_DKT_tab.join(rh_DKT_area_tab,how='left'); \n",
    "    seg_DKT_tab=seg_DKT_tab.join(lh_DKT_curv_tab,how='left'); seg_DKT_tab=seg_DKT_tab.join(rh_DKT_curv_tab,how='left'); \n",
    "    # return data\n",
    "    group_data[GROUPS[i_group]]={'Des': seg_Des_tab, 'DKT':seg_DKT_tab}\n",
    "all_data = {'Des': pd.concat([group_data['ET']['Des'], group_data['NC']['Des']]), 'DKT': pd.concat([group_data['ET']['DKT'], group_data['NC']['DKT']])}\n",
    "for k, v in all_data.items():\n",
    "    v.index   = [x.replace('-','_') for x in v.index]\n",
    "    v.columns = [x.replace('-','_') for x in v.columns]\n",
    "for x in GROUPS:\n",
    "    print(x , len(group_data[x]['Des']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82964c19-8a60-4a6d-acdc-078179e42219",
   "metadata": {},
   "source": [
    "## Generate subject list for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe9a03-7ab6-45eb-ae23-bc2cffd5fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate files for QA\n",
    "# output QA subject list \n",
    "mdd_qa_file   = main_dir / \"scripts\" / \"QA\" / 'runningMDD_qa_list.csv'  # Information from download database.\n",
    "data_df.to_csv(mdd_qa_file, columns=[], header=False)\n",
    "#generate the subject list for freesurfer results collection\n",
    "print(\"Subject list space seperated: \")\n",
    "print(' '.join((data_df.index)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
